{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aadharc/Policy-Gradient_1/blob/master/Policy_Gradient_Exercise_Zurich_SS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IFI Summer School - Foundations of Reinforcement Learning (Policy Gradient)\n",
        "\n",
        "Here we will cover the implementation of a basic reinforcement learning agent with Policy Gradient\n",
        "\n",
        "The goal of this exercise is to implement the policy gradient theorem, taking the gradient of the discounted returns under the current policy. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The key parts of the implementation that you need to fill in: Implement the loss computation - compute the returns, compute the log probabilities, compute the surrogate objective and baseline loss\n",
        "\n",
        "More details are found inline - fill out the sections labeled with [FILL THIS OUT]. \n",
        "\n",
        "First try this on the point environment, then try it on reacher (change env_name to 'point'/'reacher' in the Perform Training block)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZgUfWHCTrTar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get all dependencies\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import collections\n",
        "from datetime import datetime\n",
        "import functools\n",
        "import math\n",
        "import time\n",
        "from typing import Any, Callable, Dict, Optional, Sequence, List\n",
        "\n",
        "try:\n",
        "  import brax\n",
        "except ImportError:\n",
        "  !pip install git+https://github.com/google/brax.git@main\n",
        "  clear_output()\n",
        "  import brax\n",
        "\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "from brax.io import metrics\n",
        "from brax.training.agents.ppo import train as ppo\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# additional transfer colab dependencies\n",
        "import copy\n",
        "from typing import Tuple, Optional, Union\n",
        "from IPython.display import HTML, Image\n",
        "from jax import numpy as jnp\n",
        "from brax import jumpy as jp\n",
        "from brax.io import html, image\n",
        "from brax.envs import env as brax_env\n",
        "from brax.envs import wrappers as brax_wrappers\n",
        "from brax.envs import env\n",
        "\n",
        "\n",
        "# have torch allocate on device first, to prevent JAX from swallowing up all the\n",
        "# GPU memory. By default JAX will pre-allocate 90% of the available GPU memory:\n",
        "# https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n",
        "v = torch.ones(1, device='cuda')"
      ],
      "metadata": {
        "id": "LbR91IB2rZSb",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWJE4b5BHeH7"
      },
      "outputs": [],
      "source": [
        "#@title [FILL THIS OUT] Define REINFORCE agent\n",
        "class REINFORCEAgent(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               policy_layers: Sequence[int],\n",
        "               value_layers: Sequence[int],\n",
        "               discount: float, \n",
        "               entropy_weight: float,\n",
        "               device: str):\n",
        "    super(REINFORCEAgent, self).__init__()\n",
        "\n",
        "    # Policy definition\n",
        "    policy = []\n",
        "    for w1, w2 in zip(policy_layers, policy_layers[1:]):\n",
        "      policy.append(nn.Linear(w1, w2))\n",
        "      policy.append(nn.SiLU())\n",
        "    policy.pop()  # drop the final activation\n",
        "    self.policy = nn.Sequential(*policy)\n",
        "\n",
        "    # Baseline definition\n",
        "    value = []\n",
        "    for w1, w2 in zip(value_layers, value_layers[1:]):\n",
        "      value.append(nn.Linear(w1, w2))\n",
        "      value.append(nn.SiLU())\n",
        "    value.pop()  # drop the final activation\n",
        "    self.value = nn.Sequential(*value)\n",
        "    self.discount = discount \n",
        "    self.entropy_weight = entropy_weight\n",
        "    self.device = device\n",
        "\n",
        "  # Distributional definitions (don't need to modify)\n",
        "  @torch.jit.export\n",
        "  def dist_create(self, logits):\n",
        "    \"\"\"Normal followed by tanh.\n",
        "\n",
        "    torch.distribution doesn't work with torch.jit, so we roll our own.\"\"\"\n",
        "    loc, scale = torch.split(logits, logits.shape[-1] // 2, dim=-1)\n",
        "    scale = F.softplus(scale) + .001\n",
        "    return loc, scale\n",
        "\n",
        "  # Distributional definitions (don't need to modify)\n",
        "  @torch.jit.export\n",
        "  def dist_sample_no_postprocess(self, loc, scale):\n",
        "    return torch.normal(loc, scale)\n",
        "\n",
        "  # Distributional definitions (don't need to modify)\n",
        "  @torch.jit.export\n",
        "  def dist_entropy(self, loc, scale):\n",
        "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "    entropy = 0.5 + log_normalized\n",
        "    entropy = entropy * torch.ones_like(loc)\n",
        "    return entropy.sum(dim=-1)\n",
        "\n",
        "  # Distributional definitions (don't need to modify)\n",
        "  @torch.jit.export\n",
        "  def dist_log_prob(self, loc, scale, dist):\n",
        "    log_unnormalized = -0.5 * ((dist - loc) / scale).square()\n",
        "    log_normalized = 0.5 * math.log(2 * math.pi) + torch.log(scale)\n",
        "    log_prob = log_unnormalized - log_normalized\n",
        "    return log_prob.sum(dim=-1)\n",
        "\n",
        "  # Distributional definitions (don't need to modify)\n",
        "  @torch.jit.export\n",
        "  def get_logits_action(self, observation):\n",
        "    logits = self.policy(observation)\n",
        "    loc, scale = self.dist_create(logits)\n",
        "    action = self.dist_sample_no_postprocess(loc, scale)\n",
        "    entropy = self.dist_entropy(loc,  scale)\n",
        "    log_prob = self.dist_log_prob(loc,  scale, action)\n",
        "    return logits, action, entropy, log_prob\n",
        "\n",
        "  # Goal here is to fill out the policy gradient loss function \n",
        "  # 1. First compute the return to go from every state in a trajectory \n",
        "  #    Use this formula for Return (R_t = \\sum_{t'=t}^T \\gamma^{t'-t} r(s_{t'}, a_{t'})\n",
        "  # 2. Compute the baseline at every time step by running s_t through self.value.  \n",
        "  # 3. Next normalize the returns by subtracting mean and dividing by std (R - R_mean)/R_std\n",
        "  # 4. Compute surrogate policy loss function as \\sum_{i=1}^N \\sum_{t=0}^T log \\pi(a_t|s_t)(R_t - b)\n",
        "  #    The gradient of this loss function is taken in the outside training loop. \n",
        "  # Some tips: \n",
        "  # Go backwards through the trajectory to compute returns \n",
        "  # Use mean squared error as loss function for the baseline mean((b(s_t) - R_t)**2)\n",
        "  # When normalizing returns, make sure to add a small negative number in the denominator\n",
        "  \n",
        "  # TODO: Fill this out\n",
        "  @torch.jit.export\n",
        "  def update_parameters(self, sample_trajs: List[torch.Tensor]):\n",
        "      states = sample_trajs[0]\n",
        "      actions = sample_trajs[1]\n",
        "      rewards = sample_trajs[2]\n",
        "      entropies = sample_trajs[3]\n",
        "      log_probs = sample_trajs[4]\n",
        "\n",
        "      # Compute returns \n",
        "      # TODO\n",
        "\n",
        "      # Normalize baseline subtracted returns R_norm = (R_t - b) - mean((R_t - b))/std((R_t - b))\n",
        "      # TODO\n",
        "\n",
        "      # Compute the surrogate loss function as mean(log pi*R_norm)\n",
        "\n",
        "      # loss = TODO # Fill this out\n",
        "      return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3y5o7-oSBm-",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Sampling and batching environment data:\n",
        "def sample_trajectory(agent, env, num_steps):\n",
        "  \"\"\"Return step data over multple unrolls.\"\"\"\n",
        "  observation = env.reset()\n",
        "  states = []\n",
        "  actions = []\n",
        "  rewards = []\n",
        "  entropies = []\n",
        "  log_probs = []\n",
        "  for _ in range(num_steps):\n",
        "    logits, action, entropy, log_prob = agent.get_logits_action(observation)\n",
        "    next_observation, reward, done, info = env.step(action)\n",
        "    states.append(observation[None])\n",
        "    actions.append(action[None])\n",
        "    rewards.append(reward[None])\n",
        "    entropies.append(entropy[None])\n",
        "    log_probs.append(log_prob[None])\n",
        "    observation = next_observation\n",
        "  return [torch.transpose(torch.cat(states), 0, 1), \n",
        "          torch.transpose(torch.cat(actions), 0, 1),\n",
        "          torch.transpose(torch.cat(rewards), 0, 1), \n",
        "          torch.transpose(torch.cat(entropies), 0, 1),\n",
        "          torch.transpose(torch.cat(log_probs), 0, 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Wx_O3CAoPKm",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Task wrapper\n",
        "\n",
        "class TaskWrapper(brax_env.Wrapper):\n",
        "\n",
        "  def __init__(self, env, hide_target=False, \n",
        "               num_positions=2, setting=None, norandom_sampling=True):\n",
        "    super().__init__(env)\n",
        "    self.num_positions = num_positions\n",
        "    self.hide_target = hide_target\n",
        "    self.setting = setting\n",
        "\n",
        "    def target_positions_sampling(num_positions, rng):\n",
        "      \"\"\"Returns random target locations in a random circle slightly above xy plane.\"\"\"\n",
        "      targets = []\n",
        "      for i in range(num_positions):\n",
        "        rng, rng1, rng2 = jp.random_split(rng, 3)\n",
        "        dist = .2 * jp.random_uniform(rng1)\n",
        "        ang = jp.pi * 2. * jp.random_uniform(rng2)\n",
        "        target_x = dist * jp.cos(ang)\n",
        "        target_y = dist * jp.sin(ang)\n",
        "        if norandom_sampling:\n",
        "          target_x = 0.14142135623730953\n",
        "          target_y = 0.14142135623730953\n",
        "        target_z = .01\n",
        "        target = jp.array([target_x, target_y, target_z]).transpose()\n",
        "        targets.append(target)\n",
        "      return jnp.array(targets), rng\n",
        "\n",
        "    def deterministic_target_position(setting):\n",
        "      \"\"\"Returns a specific target location in a random circle slightly above xy plane.\"\"\"\n",
        "      if setting == 1:\n",
        "        v1 = .8\n",
        "        v2 = .5\n",
        "      elif setting == 2:\n",
        "        v1 = .4\n",
        "        v2 = .0\n",
        "      elif setting == 3:\n",
        "        v1 = .8\n",
        "        v2 = .2\n",
        "      else:\n",
        "        raise NotImplementedError('Only settings 1-4 are implemented.')\n",
        "\n",
        "      dist = .2 * v1\n",
        "      ang = jp.pi * 2. * v2\n",
        "      target_x = dist * jp.cos(ang)\n",
        "      target_y = dist * jp.sin(ang)\n",
        "      target_z = .01\n",
        "      target = jp.array([target_x, target_y, target_z]).transpose()\n",
        "      \n",
        "      return jnp.array([target])\n",
        "\n",
        "    if setting == None:\n",
        "      rng = jp.random_prngkey(seed=42)\n",
        "      self.target_positions, _ = target_positions_sampling(\n",
        "        self.num_positions, rng)\n",
        "    else:\n",
        "      self.target_positions = deterministic_target_position(setting)\n",
        "\n",
        "  def new_target_position(self, rng: jp.ndarray) -> Tuple[jp.ndarray, jp.ndarray]:\n",
        "    \"\"\"Returns a target location in a random circle slightly above xy plane.\"\"\"\n",
        "    rng, rng1  = jp.random_split(rng, 2)\n",
        "\n",
        "    index = jp.randint(rng1, low = 0, high = self.num_positions)\n",
        "    target = self.target_positions[index]\n",
        "\n",
        "    return rng, target\n",
        "\n",
        "  def reset(self, rng: jp.ndarray) -> brax_env.State:\n",
        "    rng, rng1, rng2 = jp.random_split(rng, 3)\n",
        "\n",
        "    self_sys = self.sys \n",
        "    qpos = self_sys.default_angle() + jp.random_uniform(\n",
        "        rng1, (self_sys.num_joint_dof,), -.1, .1)\n",
        "    qvel = jp.random_uniform(rng2, (self_sys.num_joint_dof,), -.005, .005)\n",
        "\n",
        "    qp = self_sys.default_qp(joint_angle=qpos, joint_velocity=qvel)\n",
        "    _, target = self.new_target_position(rng)\n",
        "    pos = jp.index_update(qp.pos, self._target_idx, target)\n",
        "\n",
        "    qp = qp.replace(pos=pos)\n",
        "    obs, _ = self._get_obs(qp, self_sys.info(qp))\n",
        "    reward, done, zero = jp.zeros(3)\n",
        "    metrics = {\n",
        "        'reward_dist': zero,\n",
        "        'reward_ctrl': zero,\n",
        "    }\n",
        "    return brax_env.State(qp, obs, reward, done, metrics)\n",
        "\n",
        "  def step(self, state, action):\n",
        "    qp, info = self.sys.step(state.qp, action)\n",
        "    obs, obs_full = self._get_obs(qp, info)\n",
        "\n",
        "    # vector from tip to target is last 3 entries of obs vector\n",
        "    reward_dist = -jp.norm(obs_full[-3:])\n",
        "    reward_ctrl = -jp.square(action).sum()\n",
        "    reward = reward_dist + reward_ctrl\n",
        "\n",
        "    state.metrics.update(\n",
        "        reward_dist=reward_dist,\n",
        "        reward_ctrl=reward_ctrl,\n",
        "    )\n",
        "\n",
        "    return state.replace(qp=qp, obs=obs, reward=reward)\n",
        "\n",
        "  def _get_obs(self, qp: brax.QP, info: brax.Info) -> jp.ndarray:\n",
        "    \"\"\"Egocentric observation of target and arm body.\"\"\"\n",
        "    # (joint_angle,), _ = self.sys.joints[0].angle_vel(qp)\n",
        "    joint_angle, _ = self.sys.joints[0].angle_vel(qp)\n",
        "\n",
        "    # qpos:\n",
        "    # x,y coord of target\n",
        "    qpos = [qp.pos[self._target_idx, :2]]\n",
        "\n",
        "    # dist to target and speed of tip\n",
        "    arm_qps = jp.take(qp, jp.array(self._arm_idx))\n",
        "    tip_pos, tip_vel = arm_qps.to_world(jp.array([0.11, 0., 0.]))\n",
        "    tip_to_target = [tip_pos - qp.pos[self._target_idx]]\n",
        "    cos_sin_angle = [jp.cos(joint_angle), jp.sin(joint_angle)]\n",
        "\n",
        "    # qvel:\n",
        "    # velocity of tip\n",
        "    qvel = [tip_vel[:2]]\n",
        "\n",
        "    if self.hide_target:\n",
        "      # return used and full observation\n",
        "      return (jp.concatenate(cos_sin_angle + qvel), \n",
        "              jp.concatenate(cos_sin_angle + qpos + qvel + tip_to_target))\n",
        "    else:\n",
        "      return (jp.concatenate(cos_sin_angle + qpos + qvel + tip_to_target),\n",
        "              jp.concatenate(cos_sin_angle + qpos + qvel + tip_to_target))\n",
        "    \n",
        "  @property\n",
        "  def observation_size(self) -> int:\n",
        "    \"\"\"The size of the observation vector returned in step and reset.\"\"\"\n",
        "    rng = jp.random_prngkey(0)\n",
        "    reset_state = self.reset(rng)\n",
        "    return reset_state.obs.shape[-1]\n",
        "\n",
        "#@title Additional environment definitions\n",
        "\n",
        "def create_reacher_env(\n",
        "           episode_length: int = 1000,\n",
        "           action_repeat: int = 1,\n",
        "           auto_reset: bool = True,\n",
        "           batch_size: Optional[int] = None,\n",
        "           eval_metrics: bool = False,\n",
        "           task_wrapper: bool = False,\n",
        "           num_positions = 1,\n",
        "           hide_target = False,\n",
        "           setting = None,\n",
        "           **kwargs):\n",
        "  \"\"\"Creates an Env with a specified brax system.\"\"\"\n",
        "  env = brax.envs.reacher.Reacher(**kwargs)\n",
        "  if task_wrapper:\n",
        "    env = TaskWrapper(env, num_positions=num_positions, \n",
        "                      hide_target=hide_target, setting=setting)\n",
        "  if episode_length is not None:\n",
        "    env = brax_wrappers.EpisodeWrapper(env, episode_length, action_repeat)\n",
        "  if batch_size:\n",
        "    env = brax_wrappers.VectorWrapper(env, batch_size)\n",
        "  if auto_reset:\n",
        "    env = brax_wrappers.AutoResetWrapper(env)\n",
        "  if eval_metrics:\n",
        "    env = brax_wrappers.EvalWrapper(env)\n",
        "\n",
        "  return env  # type: ignore\n",
        "\n",
        "def create_gym_reacher_env(\n",
        "                   batch_size: Optional[int] = None,\n",
        "                   seed: int = 0,\n",
        "                   backend: Optional[str] = None,\n",
        "                   **kwargs) -> Union[gym.Env, gym.vector.VectorEnv]:\n",
        "  \"\"\"Creates a `gym.Env` or `gym.vector.VectorEnv` from a Brax environment.\"\"\"\n",
        "  environment = create_reacher_env(batch_size=batch_size, **kwargs)\n",
        "  if batch_size is None:\n",
        "    return brax_wrappers.GymWrapper(environment, seed=seed, backend=backend)\n",
        "  if batch_size <= 0:\n",
        "    raise ValueError(\n",
        "        '`batch_size` should either be None or a positive integer.')\n",
        "  return brax_wrappers.VectorGymWrapper(environment, seed=seed, backend=backend)\n",
        "\n",
        "\n",
        "class PointMass(env.Env):\n",
        "  \"\"\"Trains an agent to go fast.\"\"\"\n",
        "\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(config='dt: .02', **kwargs)\n",
        "\n",
        "  def reset(self, rng: jnp.ndarray) -> brax_env.State:\n",
        "    qp = brax.QP(pos=jnp.zeros(2), vel=jnp.zeros(2), rot=jnp.zeros(2), ang=jnp.zeros(2))\n",
        "    obs = jnp.zeros(2)\n",
        "    goal = jnp.ones(2)\n",
        "    reward, done = jnp.zeros(2)\n",
        "    return brax_env.State(qp, obs, reward, done, info={'goal': goal})\n",
        "\n",
        "  def step(self, state: env.State, action: jnp.ndarray) -> brax_env.State:\n",
        "    pos = state.qp.pos + action * self.sys.config.dt\n",
        "    qp = state.qp.replace(pos=pos)\n",
        "    obs = pos.copy()\n",
        "    reward = -((state.info[\"goal\"][0] - pos[0])**2 + \\\n",
        "               (state.info[\"goal\"][1]- pos[1])**2)\n",
        "    return state.replace(qp=qp, obs=obs, reward=reward)\n",
        "\n",
        "  @property\n",
        "  def observation_size(self):\n",
        "    return 2\n",
        "\n",
        "  @property\n",
        "  def action_size(self):\n",
        "    return 2\n",
        "\n",
        "#@title Additional environment definitions\n",
        "def create_point_env(\n",
        "           episode_length: int = 1000,\n",
        "           action_repeat: int = 1,\n",
        "           auto_reset: bool = True,\n",
        "           batch_size: Optional[int] = None,\n",
        "           eval_metrics: bool = False,\n",
        "           **kwargs):\n",
        "  \"\"\"Creates an Env with a specified brax system.\"\"\"\n",
        "  env = PointMass(**kwargs)\n",
        "  if episode_length is not None:\n",
        "    env = brax_wrappers.EpisodeWrapper(env, episode_length, action_repeat)\n",
        "  if batch_size:\n",
        "    env = brax_wrappers.VectorWrapper(env, batch_size)\n",
        "  if auto_reset:\n",
        "    env = brax_wrappers.AutoResetWrapper(env)\n",
        "  if eval_metrics:\n",
        "    env = brax_wrappers.EvalWrapper(env)\n",
        "\n",
        "  return env  # type: ignore\n",
        "\n",
        "def create_gym_point_env(\n",
        "                   batch_size: Optional[int] = None,\n",
        "                   seed: int = 0,\n",
        "                   backend: Optional[str] = None,\n",
        "                   **kwargs) -> Union[gym.Env, gym.vector.VectorEnv]:\n",
        "  \"\"\"Creates a `gym.Env` or `gym.vector.VectorEnv` from a Brax environment.\"\"\"\n",
        "  environment = create_point_env(batch_size=batch_size, **kwargs)\n",
        "  if batch_size is None:\n",
        "    return brax_wrappers.GymWrapper(environment, seed=seed, backend=backend)\n",
        "  if batch_size <= 0:\n",
        "    raise ValueError(\n",
        "        '`batch_size` should either be None or a positive integer.')\n",
        "  return brax_wrappers.VectorGymWrapper(environment, seed=seed, backend=backend)\n",
        "\n",
        "def make_env(env_name: str = 'reacher', \n",
        "            num_envs: int = 2048,\n",
        "            episode_length: int = 100,\n",
        "            device = 'cuda',\n",
        "            num_target_positions = 1,\n",
        "            hide_target = False,\n",
        "            setting = None):\n",
        "  if env_name =='reacher':\n",
        "    env = create_gym_reacher_env(task_wrapper=True, \n",
        "                                num_positions=num_target_positions, \n",
        "                                hide_target=hide_target,\n",
        "                                setting=setting,\n",
        "                                batch_size=num_envs,\n",
        "                                episode_length=episode_length)\n",
        "  elif env_name == 'point':\n",
        "      env = create_gym_point_env(batch_size=num_envs,\n",
        "                             episode_length=episode_length)\n",
        "\n",
        "  # automatically convert between jax ndarrays and torch tensors:\n",
        "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "\n",
        "  return env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAnGdkfQoPKo",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Define training loop\n",
        "\n",
        "def train(\n",
        "    env_name: str = 'reacher',\n",
        "    num_envs: int = 2048,\n",
        "    episode_length: int = 100,\n",
        "    device: str = 'cuda',\n",
        "    num_epochs: int = 200,\n",
        "    discount: float = 0.99,\n",
        "    entropy_weight: float = 0.0001, \n",
        "    hidden_size: int = 128,\n",
        "    learning_rate: float = 3e-4,\n",
        "    num_update_steps = 1\n",
        "):\n",
        "\n",
        "  # Define environment  \n",
        "  env = make_env(env_name, num_envs, episode_length, device)\n",
        "    \n",
        "  # env warmup\n",
        "  env.reset()\n",
        "  action = torch.zeros(env.action_space.shape).to(device)\n",
        "  env.step(action)\n",
        "\n",
        "  # create the agent\n",
        "  policy_layers = [env.observation_space.shape[-1], hidden_size, hidden_size, env.action_space.shape[-1] * 2]\n",
        "  value_layers = [env.observation_space.shape[-1], hidden_size, hidden_size, 1]\n",
        "  agent = REINFORCEAgent(policy_layers, value_layers, discount, entropy_weight, device)\n",
        "  agent = torch.jit.script(agent.to(device))\n",
        "  optimizer = optim.Adam(agent.parameters())\n",
        "    \n",
        "  # Bookkeeping\n",
        "  returns = []\n",
        "\n",
        "  # Training loop\n",
        "  for iter_num in range(num_epochs):\n",
        "    # Sample trajectories\n",
        "    sample_trajs = sample_trajectory(agent, env, episode_length)\n",
        "    rewards_np = sample_trajs[2].cpu().numpy().sum(axis=-1).mean()\n",
        "    print(\"Episode: {}, reward: {}\".format(iter_num, rewards_np))\n",
        "    returns.append(rewards_np)\n",
        "\n",
        "    # Perform actor-critic update\n",
        "    for update_num in range(num_update_steps):\n",
        "        loss = agent.update_parameters(sample_trajs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "  plt.plot(returns)\n",
        "  return env, agent"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Perform training\n",
        "# TODO: Adjust parameters if necessary\n",
        "env_name = 'pointer'\n",
        "env, agent = train(env_name=env_name, \n",
        "                   num_envs=1000, \n",
        "                   episode_length=200, \n",
        "                   num_epochs=200)"
      ],
      "metadata": {
        "id": "RuujDrFNyrpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize agent if point\n",
        "\n",
        "all_obs = []\n",
        "all_rewards = []\n",
        "observation = env.reset()\n",
        "for _ in range(200):\n",
        "  all_obs.append(observation[:, None, :])\n",
        "  logits, action, entropy, log_prob = agent.get_logits_action(observation)\n",
        "  observation, reward, done, info = env.step(action) \n",
        "  all_rewards.append(reward[:, None])\n",
        "all_obs = torch.cat(all_obs, dim=1)\n",
        "all_obs = all_obs.cpu().detach().numpy()\n",
        "\n",
        "plt.clf()\n",
        "plt.cla()\n",
        "for j in range(1000):\n",
        "  plt.plot(all_obs[j, :, 0],  all_obs[j, :, 1])\n",
        "plt.scatter(1, 1, marker='x', s=20)\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-pbsHQtzRii6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-lrKHvkUeYM"
      },
      "outputs": [],
      "source": [
        "#@title Perform training\n",
        "# TODO: Adjust parameters if necessary\n",
        "env_name = 'reacher'\n",
        "env, agent = train(env_name=env_name, \n",
        "                   num_envs=1000, \n",
        "                   episode_length=200, \n",
        "                   num_epochs=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Visualize agent if reacher\n",
        "\n",
        "def visualise_agent(env, agent, episode_length, num_episodes=3):\n",
        "  episodes = []\n",
        "  for i in range(num_episodes):\n",
        "    episodes.append([])\n",
        "\n",
        "  observation = env.reset()\n",
        "  for _ in range(episode_length):\n",
        "    logits, action, entropy, log_prob = agent.get_logits_action(observation)\n",
        "    observation, reward, done, info = env.step(action)    \n",
        "    batch_state = env.env._state\n",
        "    for i in range(num_episodes):\n",
        "      episodes[i].append(brax.QP(batch_state.qp.pos[i], batch_state.qp.rot[i],\n",
        "                                batch_state.qp.vel[i], batch_state.qp.ang[i]))\n",
        "      \n",
        "  rollout = []\n",
        "  [rollout.extend(ep) for ep in episodes] \n",
        "\n",
        "  return rollout\n",
        "\n",
        "rollouts = visualise_agent(env, agent, episode_length=200, num_episodes=10)\n",
        "HTML(html.render(env.env._env.sys, rollouts))"
      ],
      "metadata": {
        "id": "C7ChbozpqyMU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}